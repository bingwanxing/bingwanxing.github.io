<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HELLO WORLD</title>
    <url>/2024/10/10/HELLO%20WORLD/</url>
    <content><![CDATA[<p>引言：</p>
<p>悠古的回响寥寥，夏娃亚当偷尝禁果的欢欣仍刺激着无数人，<br>
时间流过，我看不见神，<br>
胸中依然感到青春的震荡，<br>
沐浴着新生的水，<br>
“HELLO WORLD”</p>
<p>This blog will contain my code stuff and learning process…and maybe more.</p>
<p>这个博客会包含我的代码相关和学习历程…也许还有别的。</p>
]]></content>
  </entry>
  <entry>
    <title>On-device LLM Deployment-2</title>
    <url>/2024/10/18/LLM-Deployment-on-Android-2/</url>
    <content><![CDATA[<p>After last week’s exploration,I got more <a href="http://practised.So">practised.So</a> I have basically done the Deployment on Android,which should be working but got a problem.The problem is every time I open the app containing the Gemma-2-2b,the stimulater got crashed.</p>
<p>I think it’s because the configuration of stimulater is not <a href="http://enough.So">enough.So</a> there is two ways to solve it:1,use a real phone;2,finetune the model.</p>
<p>I choose the latter.</p>
<p>So the way became so clear,now my work is to use some finetune strategies to make the model works better.</p>
<p>Firstly I tried LoRA.And I will try DV and prunning next week.(no Qutization because the model has already been 8-bit.)(now I found this idea is wrong:d.)</p>
<p>And here’s summary of four ways to optimize LLMs:</p>
<h2 id="quantization">Quantization</h2>
<p><strong>Quantization</strong> minimizes the precision (number after decimal) of model parameters, leading to reduced memory usage and faster computation, which is vital for deploying LLMs on devices with limited resources.</p>
<p>Trade-off: While quantization offers efficiency gains, a potential downside is a slight decrease in model accuracy. The impact on accuracy can vary depending on the specific technique and the task at hand.</p>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>This involves training a smaller “student” model to replicate the performance of a larger “teacher” model. The student model learns to mimic the outputs of the teacher model, achieving similar results with fewer parameters.</p>
<h2 id="pruning">Pruning</h2>
<p><strong>Pruning</strong> involves removing parts of a model (such as neurons or entire layers) that contribute little to the output, effectively reducing the model size and improving computational efficiency without significantly affecting accuracy.</p>
<h2 id="tuning">Tuning</h2>
<p><strong>Finetuning</strong> involves adapting pre-trained models to specific tasks, enhancing their effectiveness without expansive retraining:<br>
Trade-off: It risks overfitting, where the model performs well on training data but poorly on unseen data.</p>
<p><strong>Last Layer Tuning</strong> adjusts the final layer of the model, optimizing it for tasks closely related to the original training. The trade off is, that it might not capture complex patterns as that require deeper model adjustments.</p>
<p><strong>Adapter Layer Tuning</strong> integrates trainable modules between layers, tailoring the model’s output. As it adds extra parameters to the model, which can increase the computational load.</p>
<p><strong>Prefix Tuning</strong> adds tunable parameters at the input sequence start, directing the model’s focus. Trade off is that it requires careful design of the prefix layer to avoid introducing biases.</p>
<p><strong>Low Rank Adaptation (LoRA)</strong> employs low-rank matrices for efficient weight adaptation.<br>
potentially impacting the model’s adaptability to highly complex tasks. Trade off is, potentially impacting the model’s adaptability to highly complex tasks.</p>
<blockquote>
<p><em>我觉得神还缺少某些某些东西，只要不存在与之相对的东西。————吕西安</em></p>
</blockquote>
]]></content>
      <tags>
        <tag>SClog</tag>
      </tags>
  </entry>
  <entry>
    <title>On-device LLM Deployment-3</title>
    <url>/2024/10/26/on-device-LLM-deployment-3/</url>
    <content><![CDATA[<p>This week focus on paper reading and basic knowledge learning.</p>
<p>First,I summarised the difference of QAT and PTQ.</p>
<h2 id="quantization-aware-training (qat) core method">Quantization-Aware Training (QAT) Core Method</h2>
<p>Simulated Quantization During Training,Full Model Fine-Tuning,Quantizing Weights and Activations.</p>
<h2 id="post-training-quantization (ptq) core method">Post-Training Quantization (PTQ) Core Method</h2>
<p>Quantization After Training,Static Quantization,Dynamic Quantization.</p>
<p>Here is a chart about comparison of them.</p>
<table>
<thead>
<tr>
<th><strong>Feature</strong></th>
<th><strong>Quantization-Aware Training(QAT)</strong></th>
<th><strong>Post-Training Quantization (PTQ)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>Requires retraining with quantization</td>
<td>No retraining needed, applied aftertraining</td>
</tr>
<tr>
<td>Use Case</td>
<td>Best for high-precision tasks on resource.constrained devices</td>
<td>Fast deployment, suited for simplerquantization tasks</td>
</tr>
<tr>
<td>Accuracy Loss</td>
<td>Minimal, close to floating-point accuracy</td>
<td>Potential for higher accuracy loss</td>
</tr>
<tr>
<td>Efficiency Gains</td>
<td>High efficiency on low-precision hardware</td>
<td>Also boosts efficiency, but lessoptimal for some models</td>
</tr>
<tr>
<td>Complexity</td>
<td>Higher complexity due to simulatedquantization during training</td>
<td>Simpler implementation</td>
</tr>
</tbody>
</table>
<p>Secondly,I go over knowledge of transfomer,whose note is on the goodnotes.</p>
<blockquote>
<p><em>我总感觉像生活在大海上，受到威胁，然而心中存有巨大的幸福。————加谬</em></p>
</blockquote>
]]></content>
      <tags>
        <tag>SClog</tag>
      </tags>
  </entry>
  <entry>
    <title>On-device LLM Deployment-1</title>
    <url>/2024/10/11/LLM-Deployment-on-Android-1/</url>
    <content><![CDATA[<p>Tried some tutorial about how to deploy a model on Android,but I got lost.Here is the process.</p>
<p>At first,I wanted to change the type of model to <a href="http://TFlite.It">TFlite.It</a> is not so easy.</p>
<p>And then,I asked a guy online for some sugggestions.He told me I can use onnx framework and so on,no need for changing the type of <a href="http://model.So">model.So</a> I began to learn some something about onnx and tried to use it,finding the framework hard to work.</p>
<p>Eventually,I tried to deploy some tiny models(like CV) following the tutorial online.</p>
<p>Anyways,this week’s exploration is such a chaos.But this week’s exploration gives me a lot of exprience and it’s necessary.</p>
<blockquote>
<p><em>没有生活的的绝望就没有对生活的爱。————加谬</em></p>
</blockquote>
]]></content>
      <tags>
        <tag>SClog</tag>
      </tags>
  </entry>
  <entry>
    <title>&#39;The-first-kaggle-experience&#39;</title>
    <url>/2025/02/26/The-first-kaggle-experience/</url>
    <content><![CDATA[<h2 id="data-process">Data Process</h2>
<p>1.after checking the data file,I found that the keyword and location column is NaN.Therefore,I fill “unkown” to replace NaN.</p>
<div class="highlight"><pre class="code"><code>data_train[<span class="hljs-string">&#x27;keyword&#x27;</span>] = data_train[<span class="hljs-string">&#x27;keyword&#x27;</span>].fillna(<span class="hljs-string">&#x27;uknown_keyword&#x27;</span>)
data_train[<span class="hljs-string">&#x27;location&#x27;</span>] = data_train[<span class="hljs-string">&#x27;location&#x27;</span>].fillna(<span class="hljs-string">&#x27;uknown_location&#x27;</span>)
</code></pre></div>
<p>2.I use Pandas to combine the keyword,location,and text columns of each row in the dataset into a single string,and stores these strings in a list.</p>
<div class="highlight"><pre class="code"><code>corpus = data_train.apply(<span class="hljs-keyword">lambda</span> row:<span class="hljs-string">f&quot;keyword:<span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;keyword&#x27;</span>]&#125;</span> | location:<span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;location&#x27;</span>]&#125;</span> | text: <span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;text&#x27;</span>]&#125;</span>&quot;</span>,axis=<span class="hljs-number">1</span>).tolist()
</code></pre></div>
<p>3.I use BertTokenizer to tokenize and preprocessing the list.</p>
<div class="highlight"><pre class="code"><code>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
X_bert=tokenizer(
    corpus,
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&#x27;tf&#x27;</span>
)
</code></pre></div>
<p>4.I use TensorFlow Dataset to transform the X.Bert to a dataset used in BERT.</p>
<div class="highlight"><pre class="code"><code>dataset = tf.data.Dataset.from_tensor_slices((
    &#123;<span class="hljs-string">&quot;input_ids&quot;</span>: input_ids, <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask&#125;,
    y
)).batch(<span class="hljs-number">32</span>)
</code></pre></div>
<h2 id="model-train">Model Train</h2>
<p>I choose BERT and set</p>
<div class="highlight"><pre class="code"><code>optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>]
</code></pre></div>
<p><strong>Why?</strong></p>
<p>This chanllenge is a classification problem,and BERT has these suitable features:<br>
1.Superior Understanding of Context from Both Directions;<br>
2.Effectiveness in Pre-training and Fine-tuning;<br>
3.Deep Contextual Analysis;<br>
4.Versatility Across Multiple Tasks</p>
<h2 id="limitation">Limitation</h2>
<p>1.Long Training Time:I only trained for 3 epochs,but it took 12719.4s.<br>
2.Not so superior performance:the final score is only 0.57033:Perhaps this is because BERT may not be as effective as traditional machine learning models (e.g., Naive Bayes) or shallow neural networks (e.g., CNNs) for some simple text classification tasks, such as short text classification or binary classification problems, and they also have advantages in terms of computing resources and training time.</p>
<h2 id="to-do">TO DO</h2>
<p>1.Increase the train epochs;<br>
2.Try other models or methods;<br>
3.Add error blog.</p>
]]></content>
  </entry>
  <entry>
    <title>The-first-kaggle-experience-update</title>
    <url>/2025/03/19/The-first-kaggle-experience-update/</url>
    <content><![CDATA[<h2 id="data-process">Data Process</h2>
<p>1.after checking the data file,I found that the keyword and location column is NaN.Therefore,I fill “unkown” to replace NaN.</p>
<div class="highlight"><pre class="code"><code>data_train[<span class="hljs-string">&#x27;keyword&#x27;</span>] = data_train[<span class="hljs-string">&#x27;keyword&#x27;</span>].fillna(<span class="hljs-string">&#x27;uknown_keyword&#x27;</span>)
data_train[<span class="hljs-string">&#x27;location&#x27;</span>] = data_train[<span class="hljs-string">&#x27;location&#x27;</span>].fillna(<span class="hljs-string">&#x27;uknown_location&#x27;</span>)
</code></pre></div>
<p>2.I use Pandas to combine the keyword,location,and text columns of each row in the dataset into a single string,and stores these strings in a list.</p>
<div class="highlight"><pre class="code"><code>corpus = data_train.apply(<span class="hljs-keyword">lambda</span> row:<span class="hljs-string">f&quot;keyword:<span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;keyword&#x27;</span>]&#125;</span> | location:<span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;location&#x27;</span>]&#125;</span> | text: <span class="hljs-subst">&#123;row[<span class="hljs-string">&#x27;text&#x27;</span>]&#125;</span>&quot;</span>,axis=<span class="hljs-number">1</span>).tolist()
</code></pre></div>
<p>3.I use BertTokenizer to tokenize and preprocessing the list.</p>
<div class="highlight"><pre class="code"><code>max_len = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> corpus:
    <span class="hljs-comment"># 将文本分词，并添加 `[CLS]` 和 `[SEP]` 符号</span>
    input_ids = tokenizer.encode(sent, add_special_tokens=<span class="hljs-literal">True</span>)
    max_len = <span class="hljs-built_in">max</span>(max_len, <span class="hljs-built_in">len</span>(input_ids))
    
<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Max sentence length: &#x27;</span>, max_len)

max_len = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> corpuss:
    input_ids = tokenizer.encode(sent, add_special_tokens=<span class="hljs-literal">True</span>)
    max_len = <span class="hljs-built_in">max</span>(max_len, <span class="hljs-built_in">len</span>(input_ids))
    
<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Max sentence length: &#x27;</span>, max_len)

input_ids = []
attention_masks = []

<span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> corpus:
    X_bert = tokenizer.encode_plus(
        sent,
        add_special_tokens = <span class="hljs-literal">True</span>,
        max_length = <span class="hljs-number">128</span>,
        pad_to_max_length = <span class="hljs-literal">True</span>,
        return_attention_mask = <span class="hljs-literal">True</span>,
        return_tensors = <span class="hljs-string">&#x27;pt&#x27;</span>,
    )
    input_ids.append(X_bert[<span class="hljs-string">&#x27;input_ids&#x27;</span>])
    attention_masks.append(X_bert[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])

input_ids = torch.cat(input_ids,dim=<span class="hljs-number">0</span>)
attention_masks = torch.cat(attention_masks,dim=<span class="hljs-number">0</span>)
labels = torch.tensor(y, dtype=torch.long)

<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Original: &#x27;</span>, corpus[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Token IDs:&#x27;</span>, input_ids[<span class="hljs-number">0</span>])

input_idss = []
attention_maskss = []

<span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> corpuss:
    encoded_dicts = tokenizer.encode_plus(
                        sent,                      
                        add_special_tokens = <span class="hljs-literal">True</span>, 
                        max_length = <span class="hljs-number">128</span>,           
                        pad_to_max_length = <span class="hljs-literal">True</span>,
                        return_attention_mask = <span class="hljs-literal">True</span>,   
                        return_tensors = <span class="hljs-string">&#x27;pt&#x27;</span>,     
                   )
    input_idss.append(encoded_dicts[<span class="hljs-string">&#x27;input_ids&#x27;</span>])
    attention_maskss.append(encoded_dicts[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])

input_idss = torch.cat(input_idss, dim=<span class="hljs-number">0</span>)
attention_maskss = torch.cat(attention_maskss, dim=<span class="hljs-number">0</span>)
</code></pre></div>
<p>4.I use pytorch Dataset to transform the tokenized corpus and corpuss to a dataset used in BERT.</p>
<div class="highlight"><pre class="code"><code><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset

dataset = TensorDataset(input_ids, attention_masks, labels)
datasets = TensorDataset(input_idss, attention_maskss)<span class="hljs-comment">#dataset for test doesn&#x27;t include labels</span>
</code></pre></div>
<h2 id="model-train">Model Train</h2>
<p>I choose BERT and set</p>
<div class="highlight"><pre class="code"><code>optimizer = AdamW(model.parameters(),
                 lr = <span class="hljs-number">2e-5</span>,
                 eps = <span class="hljs-number">1e-8</span>
                 )

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_linear_schedule_with_warmup

epochs = <span class="hljs-number">2</span>

total_steps = <span class="hljs-built_in">len</span>(train_dataloader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer,
                                           num_warmup_steps = <span class="hljs-number">0</span>,
                                           num_training_steps = total_steps)
</code></pre></div>
<p>Then,I conduct training in small batches</p>
<div class="highlight"><pre class="code"><code>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)

seed_val = <span class="hljs-number">42</span>

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

<span class="hljs-comment"># 存储训练和评估的 loss、准确率、训练时长等统计指标, </span>
training_stats = []

total_t0 = time.time()

<span class="hljs-keyword">for</span> epoch_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, epochs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;======== Epoch &#123;:&#125; / &#123;:&#125; ========&#x27;</span>.<span class="hljs-built_in">format</span>(epoch_i + <span class="hljs-number">1</span>, epochs))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Training...&#x27;</span>)

    <span class="hljs-comment"># 统计单次 epoch 的训练时间</span>
    t0 = time.time()

    <span class="hljs-comment"># 重置每次 epoch 的训练总 loss</span>
    total_train_loss = <span class="hljs-number">0</span>

    <span class="hljs-comment"># 将模型设置为训练模式。这里并不是调用训练接口的意思</span>
    model.train()

    <span class="hljs-comment"># 训练集小批量迭代</span>
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):

        <span class="hljs-comment"># 每经过40次迭代，就输出进度信息</span>
        <span class="hljs-keyword">if</span> step % <span class="hljs-number">40</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> step == <span class="hljs-number">0</span>:
            elapsed = format_time(time.time() - t0)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;  Batch &#123;:&gt;5,&#125;  of  &#123;:&gt;5,&#125;.    Elapsed: &#123;:&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(step, <span class="hljs-built_in">len</span>(train_dataloader), elapsed))

        b_input_ids = batch[<span class="hljs-number">0</span>].to(device)
        b_input_mask = batch[<span class="hljs-number">1</span>].to(device)
        b_labels = batch[<span class="hljs-number">2</span>].to(device)
        <span class="hljs-comment"># print(b_input_ids.shape)  # 应为 (batch_size, max_length)</span>
        <span class="hljs-comment"># print(b_input_mask.shape) # 同上</span>
        <span class="hljs-comment"># print(b_labels.shape)     # 应为 (batch_size,)</span>

        model.zero_grad()<span class="hljs-comment">#每次计算梯度前将其清零，因为pytorch梯度是累加的</span>
    
        <span class="hljs-comment">#前向</span>
        <span class="hljs-comment"># loss, logits = model(b_input_ids, </span>
        <span class="hljs-comment">#                          token_type_ids=None, </span>
        <span class="hljs-comment">#                          attention_mask=b_input_mask, </span>
        <span class="hljs-comment">#                          labels=b_labels)</span>
    
        outputs = model(
            input_ids=b_input_ids,
            attention_mask=b_input_mask,
            labels=b_labels  <span class="hljs-comment"># 确保传递了 labels</span>
        )
        
        <span class="hljs-comment"># 从 outputs 中提取 loss 和 logits</span>
        loss = outputs.loss  <span class="hljs-comment"># 直接访问 loss 属性</span>
        logits = outputs.logits
    
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型返回的 loss:&quot;</span>, loss.item())
    
        <span class="hljs-comment"># 累加 loss</span>
        total_train_loss += loss.item()
    
        <span class="hljs-comment"># 反向传播</span>
        loss.backward()
    
        <span class="hljs-comment"># 梯度裁剪，避免出现梯度爆炸情况</span>
        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="hljs-number">1.0</span>)
    
        <span class="hljs-comment"># 更新参数</span>
        optimizer.step()
    
        <span class="hljs-comment"># 更新学习率</span>
        scheduler.step()

    <span class="hljs-comment"># 平均训练误差</span>
    avg_train_loss = total_train_loss / <span class="hljs-built_in">len</span>(train_dataloader)            
    
    <span class="hljs-comment"># 单次 epoch 的训练时长</span>
    training_time = format_time(time.time() - t0)
</code></pre></div>
<h2 id="model-prediction">Model Prediction</h2>
<p>I use model.eval to get the prediction result</p>
<div class="highlight"><pre class="code"><code>model.<span class="hljs-built_in">eval</span>()

<span class="hljs-comment"># Tracking variables </span>
predictions , true_labels = [], []

<span class="hljs-comment"># 预测</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> test_dataloader:
  <span class="hljs-comment"># 将数据加载到 gpu 中</span>
  batch = <span class="hljs-built_in">tuple</span>(t.to(device) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> batch)
  b_input_ids, b_input_mask = batch
  
  <span class="hljs-comment"># 不需要计算梯度</span>
  <span class="hljs-keyword">with</span> torch.no_grad():
      <span class="hljs-comment"># 前向传播，获取预测结果</span>
      outputs = model(b_input_ids,attention_mask=b_input_mask)

  logits = outputs.logits

  <span class="hljs-comment"># 将结果加载到 cpu 中</span>
  batch_logits = logits.detach().cpu().numpy()
  
  <span class="hljs-comment"># 存储预测结果和 labels</span>
  predictions.append(batch_logits)

flat_predictions = np.concatenate(predictions, axis=<span class="hljs-number">0</span>)
predicted_labels = np.argmax(flat_predictions, axis=<span class="hljs-number">1</span>)

</code></pre></div>
<p><strong>Why?</strong></p>
<p>This chanllenge is a classification problem,and BERT has these suitable features:<br>
1.Superior Understanding of Context from Both Directions;<br>
2.Effectiveness in Pre-training and Fine-tuning;What we need to do is to add an untrained neuronal layer at the end, and then train a new model to complete our classification task;<br>
3.Deep Contextual Analysis;<br>
4.Versatility Across Multiple Tasks</p>
<h2 id="limitation">Limitation</h2>
<p>Long Training Time:I only trained for 2 epochs,but it took 10941.0s.</p>
<h2 id="to-do">TO DO</h2>
<p>1.Increase the train epochs;<br>
2.Try other models or methods;<br>
3.Add error blog.</p>
]]></content>
  </entry>
</search>
